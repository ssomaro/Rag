{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "d= ['SARADHI SOMAROUTHU\\nBloomington, Indiana | 812-671-2352 | ssomaro@iu.edu\\nlinkedin.com/in/saradhi-somarouthu |github.com/ssomaro\\n Linkedin\\nEXPERIENCE\\nData Science Intern | Novelis June 2023 – Present\\nAtlanta, Georgia\\n•Architected full-cycle forecasting system for LME contract prices, spanning algorithmic formulation, configuration of automated\\npipelines on Databricks to refresh data, and deployment of the application on Azure App Service.\\n•Developed visualization and retrieval augmentation features for EagleGPT leveraging OpenAI APIs for chat completion and\\nembeddings.\\nData Science Analyst | Accenture July 2021 – Aug 2022\\nBengaluru, India\\n•Developed automated framework to forecast demand at SKU-Store-Week granularity with an accuracy of 86% for the largest\\nconsumer electronics manufacturing company.\\n•Implemented pipelines using Azure Data Factory, Databricks and Analysis Services to detect and flag anomalies in the global', 'supply chain for the Real Time Visibility and Analytics team of one of the largest FMCG Manufacturer.\\n•Designed and developed multiple complex dashboards using Power BI that queries 10 million records of data to calculate 160+\\nKPI’s related to warehousing, logistics and customer service.\\nData Analytics Consultant | Accenture July 2020 – June 2021\\nBengaluru, India\\n•Worked on a vendor management tool to optimize the expenditure on procurement by selecting the best benchmark vendors,\\nShowed opportunity cost of 1.5 million USD over 2 quarters.\\n•Developed 15+ KPI’s and visualization using ReactJs , Python (flask) to build an in-house tool - Demand planning and\\ninventory optimization watch tower.\\nTECHNICAL SKILLS\\nProgramming Languages : Python, R, SQL, PySpark, Scala\\nDeep Learning Models and Frameworks : Multilayered Feed Forward, CNN, LSTM, PyTorch, TensorFlow, Keras\\nCloud Technologies : Azure data factory, Data bricks, Analysis services, ML Studio', 'Cloud Technologies : Azure data factory, Data bricks, Analysis services, ML Studio\\nDatabases : MySQL, PostgreSQL, MongoDB, Neo4j\\nData Visualization : Power BI, Tableau, Matplotlib, Seaborn, Dash, streamlit\\nMachine Learning Competencies : Regression, Decision trees, SVM, Bagging (Random Forests), Boosting (Gradient boosting,\\nAdaptive boosting, XGBoost)\\nEDUCATION\\nIndiana University Bloomington August 2022 – May 2024\\nMaster of Science in Data Science GPA: 3.90\\n•Coursework: Data structures and Algorithms , Machine Learning, Artificial Intelligence, Statistics, Database management,\\nSoftware Engineering. Current: Data Visualization, Knowledge Graphs for LLMs, Exploratory Data Analysis\\n•Graduate Research Assistant: Performed analysis on historical auction data, auction mechanisms, and formulated keyword\\ngeneration approaches for eBay’s bidding procedures. Conducted literature review to comprehend bidding behaviors and\\nreal-time auction dynamics in the domain of sponsored search advertising.', 'real-time auction dynamics in the domain of sponsored search advertising.\\n•Graduate Teaching Assistant: Data Representation, Spring 2023\\nPROJECTS\\nBeatMe - AI Game play bot |Search algorithms, Reinforcement Learning, Game Theory\\n•Implemented an AI Bot that plays customized version of chess, Shortest path maze, 5 x 5 board game of Goder 10 seconds.\\nDreamship |Full stack MERN application\\n•Developed a Delivery Management system to choose optimal service, place, track and manage Orders integrating MFA, Google\\nmaps API and OpenAI chat completion API for enhanced chat-bot experience with Retrieval augmented generation.\\nExplainable modeling – XGBoost and ARIMA |Python, Forecasting, Time series analysis\\n•Forecasted 4 to 26 week ahead electric load demand at hourly granularity using two independent approaches - XGBoost and', 'ARIMAX with a MAPE of 6% and 8% respectively and analysed to understand the trade-offs of using a black box model over a\\ntraditional statistical model for Parameters such as model explainability, time complexity, accuracy and modularity.\\nRelevant Tag Classification using Natural language processing |NLTK, pandas, Sklearn, Beautifulsoup\\n•Built a multi-label classifying model to assign relevant tags for blogs with a 92 % single tag accuracy aggregated at blog level.\\n•Utilized BeautifulSoup to scrape the blog content, NLTK for text processing and Logistic Regression for model building.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "\n",
    "def cut_audio(input_file, output_dir, slice_duration_ms):\n",
    "    # Load the audio file\n",
    "    audio = AudioSegment.from_file(input_file)\n",
    "\n",
    "    # Calculate the number of slices\n",
    "    num_slices = len(audio) // slice_duration_ms\n",
    "\n",
    "    for i in range(num_slices):\n",
    "        # Define the start and end points for each slice\n",
    "        start_time = i * slice_duration_ms\n",
    "        end_time = (i + 1) * slice_duration_ms\n",
    "\n",
    "        # Extract the slice\n",
    "        slice = audio[start_time:end_time]\n",
    "\n",
    "        # Define the output filename\n",
    "        output_file = f\"{output_dir}/slice_{i + 1}.mp3\"\n",
    "\n",
    "        # Export the slice to a new file\n",
    "        slice.export(output_file, format=\"mp3\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pydub.audio_segment.AudioSegment object at 0x000001AA3CE2AE10>\n"
     ]
    }
   ],
   "source": [
    "#read the audiofile\n",
    "audio = AudioSegment.from_file(\"podcast.mp3\")\n",
    "\n",
    "#split the audiofile in chunks of 5 secondec\n",
    "\n",
    "print(audio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 50000 #milliseconds\n",
    "first_audio = audio[:chunk_size]\n",
    "\n",
    "cut_audio(\"podcast.mp3\", \"output\", 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    input_file = \"input_audio.mp3\"  # Replace with the path to your audio file\n",
    "    output_dir = \"output_slices\"   # Replace with the directory where you want to save the slices\n",
    "    slice_duration_ms = 5000       # Duration of each slice in milliseconds (e.g., 5000 ms = 5 seconds)\n",
    "\n",
    "    cut_audio(input_file, output_dir, slice_duration_ms)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
